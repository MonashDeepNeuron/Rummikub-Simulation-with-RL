# -*- coding: utf-8 -*-
"""Blackjack.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OmCk_0PXq_sYnlXOtGBxiHHkLfmjOJI-

# Environment Setting
"""
from __future__ import annotations
import gymnasium as gym
import numpy as np
import torch

from gymnasium import spaces

import copy

seed = 1310

"""# Agent - Q-Learning with Q-table

- State space is small → we can store Q(s,a) in a table (dict of arrays)
"""

class BlackjackAgent_QLearning_Qtable:
    def __init__(self, env, episodes=1000, eps=0.1, alpha=0.1, gamma=0.9):
        """
        Initialize the agent.
        - env: the Blackjack environment
        - episodes: number of training episodes
        - eps: epsilon for epsilon-greedy (exploration rate)
        - alpha: learning rate (step size for Q update)
        - gamma: discount factor
        """
        self.env = env
        self.episodes = episodes
        self.eps = eps
        self.alpha = alpha
        self.gamma = gamma
        self.action_space = spaces.Discrete(2)
        self.action = ["stick", "hit"]

        # Storage for Q-values (for Q-learning), can later swap to NN
        self.Q = {}

    def get_action(self, state):
        """
        Decide which action to take given a state.
        - With probability eps: pick random action (exploration)
        - Otherwise: pick best action based on Q-values (exploitation)
        Returns: action
        """
        if state not in self.Q:
          self.Q[state] = np.zeros(self.action_space.n)

        if np.random.random() < self.eps:
          action = np.random.choice(self.action_space.n)
        else:
          action = np.argmax(self.Q[state])

        return action

    def update_q(self, state, action, reward, next_state, done):
        """
        Perform the Q-learning update rule:
        Q(s,a) ← Q(s,a) + α [ r + γ max_a' Q(s',a') - Q(s,a) ]
        """
        # Initialize unseen states
        if state not in self.Q:
          self.Q[state] = np.zeros(self.action_space.n)

        if next_state not in self.Q:
          self.Q[next_state] = np.zeros(self.action_space.n)

        target = reward # Start with immediate reward

        # Add future value if game not over
        '''
        if the episode is not finished, then we also care about the future reward we might get
        `np.max(self.Q[next_state])` = best possible value I can get in the next state
        gamma helps reduce importance of far-away rewards
        add the future rewards to the immediate reward

        `self.Q[state][action]` = Q(s,a)
        This compare the current guess with the new target we have just computed
        `(target - Q(s,a))` is the error in the estimate
        multiply by learning rate `alpha` to take a small step toward fixing it
        '''
        if not done:
          target += self.gamma * np.max(self.Q[next_state])

        self.Q[state][action] += self.alpha * (target - self.Q[state][action])

    def train(self):
        """
        Main training loop:
        - For each episode:
            1. Reset environment
            2. Loop through steps until terminal state:
                - Choose action
                - Take action in env
                - Get reward + next state
                - Update Q-values
        - Track performance (e.g., average reward)

        updates Q-values, uses epsilon-greedy
        """

        episode_rewards = []

        for i in range(self.episodes):
          state, info = self.env.reset()
          done = False
          total_reward = 0
          while not done:
            action = self.get_action(state)
            next_state, reward, terminated, truncated, info = self.env.step(action)
            done = terminated or truncated
            self.update_q(state, action, reward, next_state, done)
            state = next_state
            total_reward += reward

          episode_rewards.append(total_reward)

        print(f"Episode {i+1}/{self.episodes} → Reward: {total_reward}")
        return episode_rewards

    def play(self, num_games=10):
        """
        Run the agent in evaluation mode (greedy policy only).
        Print/return results (e.g., wins, losses, draws).

        no updates, greedy only.
        """
        results = []

        for _ in range(num_games):

          state, info = self.env.reset()
          done = False
          total_reward = 0

          while not done:
            if state in self.Q:
              action = np.argmax(self.Q[state])
            else:
              action = self.env.action_space.sample()

            next_state, reward, terminated, truncated, info = self.env.step(action)
            done = terminated or truncated
            state = next_state
            total_reward += reward

          results.append(total_reward)

        # after all games, summarize
        print(f'Average reward: {np.mean(results):.4f}')
        print(f"Wins: {results.count(1)}, Losses: {results.count(-1)}, Draws: {results.count(0)}")

        return results

"""## Running the code"""


"""## Running the code for "Agent - Q-Learning with TD Search"
"""

"""
Temporal Difference Search uses lookahead planning by simulating multiple steps into the future. This allows
the Agent to make plan ahead and consider the immediate reward and potential consequences.



"""

class BlackjackAgent_TDSearch(BlackjackAgent_QLearning_Qtable):
    
    def __init__(self, env, episodes, eps=0.1, alpha=0.1, gamma=0.9,search_depth=3, rollouts=5): 
        super().__init__(env, episodes, eps, alpha, gamma)
        self.search_depth = search_depth
        self.rollouts = rollouts

    def td_search(self, next_state):
        total_value = 0

        for _ in range(self.rollouts): # number of simulations agent runs, and averaged over
            # Copy environment to simulate
            sim_env = copy.deepcopy(self.env)
            sim_state = next_state
            sim_reward = 0 # cumulative reward throughout rollout
            discount = 1
            done = False

            for _ in range(self.search_depth): # how many steps ahead the agent looks
                if sim_state not in self.Q:
                    self.Q[sim_state] = np.zeros(self.action_space.n)

                a = np.argmax(self.Q[sim_state]) # choose greediest action
                sim_state, r, terminated, truncated, _ = sim_env.step(a) # run action
                done = terminated or truncated

                sim_reward += discount * r
                discount *= self.gamma

                if done: 
                    break

            if not done: # if game is still going after depth-number of steps ahead
                # bootstrap with Q estimate at end of rollout
                if sim_state not in self.Q:
                    self.Q[sim_state] = np.zeros(self.action_space.n)
                sim_reward += discount * np.max(self.Q[sim_state])

            total_value += sim_reward

        return total_value / self.rollouts

    def update_q(self, state, action, reward, next_state, done):
        if state not in self.Q:
            self.Q[state] = np.zeros(self.action_space.n)
        if next_state not in self.Q:
            self.Q[next_state] = np.zeros(self.action_space.n)

        if done:
            target = reward
        else:
            target = reward + self.gamma * self.td_search(next_state)

        self.Q[state][action] += self.alpha * (target - self.Q[state][action])
    

if __name__ == "__main__":
    # Create Blackjack environment
    env = gym.make("Blackjack-v1", natural=False, sab=False)  # sab=False = default rules

    # Create agent
    agent = BlackjackAgent_TDSearch(
        env,
        episodes=1000,
        eps=0.1,
        alpha=0.1,
        gamma=0.9,
        search_depth=3,
        rollouts=5,
    )

    # Train the agent
    rewards = agent.train()

    # Evaluate performance
    agent.play(num_games=100)
